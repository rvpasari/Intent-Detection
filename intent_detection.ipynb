{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT INTRODUCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This intent detection problem is on some text extracted from Enron emails (It would be interesting to examine how closely related it is to their fraud back in the early 2000s, as I had the chance to study it in my accounting fraud class). The dataset is primarily divided into sentences that are classified as \"yes\" or \"no\", depending on whether they communicate some form of intent. This form of intent could fall under the categories of\"request\", \"propose\" or even \"commit\". My task is to build a mathematical/statistical model that helps in determining whether a sentence has such intent or not, and for model 1.0, I'll take two approaches to see what results I can obtain. \n",
    "\n",
    "1. Since the classification is already done, we break the dataset up into the \"yes\" and \"no\" categories, and attempt to cluster words that are common to both the classes. Our interest, of course, remains focused on the words that we obtain in the \"yes\" category, but we will examine the ones in \"no\" as well. We study the frequency and more importantly, the difference in frequency of the words, extend this analysis to the phrases in the sentences to see if we can obtain any insights.\n",
    "\n",
    "\n",
    "2. After doing so, we will attempt to use some prebuilt models to see if they are effective at detecting commonalities between the all the different \"yes\" sentences, and check them against our test dataset. Following this, we will attempt to use a more mathematically intense deep learning models (that are common to such datasets) to see if there are any changes in accuracy levels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import chart_studio.plotly as py\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(file_name):\n",
    "    data =  open(file_name, \"r\") \n",
    "    \n",
    "    punctuation = str.maketrans(\"\",\"\", string.punctuation)\n",
    "    corpus_map = {\"YES\":[], \"NO\":[]}\n",
    "    intent_map = {\"YES\":[], \"NO\":[]}\n",
    "    items = data.read().split(\"\\n\")\n",
    "    items.remove(\"\")\n",
    "    \n",
    "    #reading file line by line, adding data to a dictionary\n",
    "    for line in items:  \n",
    "        intent = line.split(\"\\t\")[0]\n",
    "        sentence = line.split(\"\\t\")[1]\n",
    "        corpus_map[intent.upper()].append(sentence.translate(punctuation))\n",
    "        intent_map[intent.upper()].append(sentence.split(\" \"))\n",
    "    print(\"NUMBER OF POSITIVE INTENTS:\",len(corpus_map['YES']), \"NUMBER OF NO INTENTS:\" ,len(corpus_map['NO']))\n",
    "    \n",
    "    #convert to lower case, remove punctuation\n",
    "    no = [x.lower().translate(punctuation) for sublist in intent_map['NO'] for x in sublist]\n",
    "    yes = [x.lower().translate(punctuation) for sublist in intent_map['YES'] for x in sublist]\n",
    "    \n",
    "    #obtain frequency of words\n",
    "    no_freq = sorted(Counter(no).items(), key=operator.itemgetter(1), reverse = True)\n",
    "    yes_freq = sorted(Counter(yes).items(), key=operator.itemgetter(1), reverse = True)\n",
    "\n",
    "    #obtain dataframe for frequency\n",
    "    no_df = pd.DataFrame(no_freq, columns =[\"word\", \"frequency\"])\n",
    "    yes_df = pd.DataFrame(yes_freq, columns = [\"word\", \"frequency\"])\n",
    "\n",
    "    #create y_train and return\n",
    "    x = corpus_map['YES'] + corpus_map['NO']\n",
    "    y = [1]*len(corpus_map['YES']) + [0]*len(corpus_map['NO'])\n",
    "    return x,y, yes_df, no_df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF POSITIVE INTENTS: 1719 NUMBER OF NO INTENTS: 1938\n",
      "NUMBER OF POSITIVE INTENTS: 309 NUMBER OF NO INTENTS: 683\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, yes_df, no_df = prepare_data(\"/home/rvp/Dropbox/intent_detection/enron_train.txt\")\n",
    "x_test, y_test, yes_test, no_test = prepare_data(\"/home/rvp/Dropbox/intent_detection/enron_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5463, 2), (3631, 2))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"No of unique words in each category\"\n",
    "no_df.shape, yes_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CURSORY GLANCE OF DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is small and a cursory glance at it reveals some key insights about the data (note that this is only possible because the data is relatively clean and labelled, as well as small. This is not a scalable way of approach when the dataset is huge, but a glance of a few samples is always highly encouraged).\n",
    "\n",
    "We notice patterns of phrases like \"please ____\" and sentences ending in questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSING COMMON WORDS AND UNIQUE WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>yes_intent</th>\n",
       "      <th>no_intent</th>\n",
       "      <th>yes_ratio</th>\n",
       "      <th>no_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>please</td>\n",
       "      <td>546.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>3.928058</td>\n",
       "      <td>0.254579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>call</td>\n",
       "      <td>199.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.102564</td>\n",
       "      <td>0.195980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>discuss</td>\n",
       "      <td>193.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>8.391304</td>\n",
       "      <td>0.119171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>let</td>\n",
       "      <td>114.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.562500</td>\n",
       "      <td>0.280702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>could</td>\n",
       "      <td>104.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>3.058824</td>\n",
       "      <td>0.326923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>meet</td>\n",
       "      <td>73.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.111111</td>\n",
       "      <td>0.123288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>lets</td>\n",
       "      <td>54.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>3.176471</td>\n",
       "      <td>0.314815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>join</td>\n",
       "      <td>48.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>talk</td>\n",
       "      <td>46.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>0.326087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>schedule</td>\n",
       "      <td>42.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>lunch</td>\n",
       "      <td>37.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>0.297297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>copy</td>\n",
       "      <td>33.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>further</td>\n",
       "      <td>32.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.555556</td>\n",
       "      <td>0.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>shall</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>complete</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>pm</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>confirm</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>sunday</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>mark</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>hesitate</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>feedback</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>tonight</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>asap</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>attend</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>works</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>minute</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>including</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>interested</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>survey</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>calendar</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>return</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>pick</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>sit</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>pipeline</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>appropriate</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>resume</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>involved</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>anyone</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>hard</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>apple</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>prepare</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>eol</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>within</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>bob</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>suggestions</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>copies</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>type</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>text</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>device</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>evening</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>social</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>response</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>approval</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>remember</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>code</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>september</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>yes</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>reach</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>mobile</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>lay</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  yes_intent  no_intent  yes_ratio  no_ratio\n",
       "4       please       546.0      139.0   3.928058  0.254579\n",
       "22        call       199.0       39.0   5.102564  0.195980\n",
       "23     discuss       193.0       23.0   8.391304  0.119171\n",
       "36         let       114.0       32.0   3.562500  0.280702\n",
       "42       could       104.0       34.0   3.058824  0.326923\n",
       "..         ...         ...        ...        ...       ...\n",
       "491  september         6.0        1.0   6.000000  0.166667\n",
       "494        yes         6.0        1.0   6.000000  0.166667\n",
       "495      reach         6.0        1.0   6.000000  0.166667\n",
       "496     mobile         6.0        1.0   6.000000  0.166667\n",
       "498        lay         6.0        1.0   6.000000  0.166667\n",
       "\n",
       "[163 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words = pd.merge(yes_df.head(500), no_df.head(500), on='word', how='outer').fillna(1)\n",
    "common_words.columns = ['Word', 'yes_intent', 'no_intent']\n",
    "common_words['yes_ratio'] = common_words['yes_intent']/common_words['no_intent']\n",
    "common_words['no_ratio'] = common_words['no_intent']/common_words['yes_intent']\n",
    "common_words[common_words['yes_ratio']>3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>yes_intent</th>\n",
       "      <th>no_intent</th>\n",
       "      <th>yes_ratio</th>\n",
       "      <th>no_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>was</td>\n",
       "      <td>24.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.303797</td>\n",
       "      <td>3.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>day</td>\n",
       "      <td>23.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>0.294872</td>\n",
       "      <td>3.391304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>receive</td>\n",
       "      <td>16.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>no</td>\n",
       "      <td>14.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>3.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>he</td>\n",
       "      <td>14.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>3.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>they</td>\n",
       "      <td>13.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>5.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>their</td>\n",
       "      <td>13.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>3.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>business</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>3.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>she</td>\n",
       "      <td>10.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>3.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>off</td>\n",
       "      <td>10.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>3.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>given</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>3.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>been</td>\n",
       "      <td>9.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>5.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>2</td>\n",
       "      <td>8.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>3.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>because</td>\n",
       "      <td>8.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>3.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>way</td>\n",
       "      <td>7.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>3.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>most</td>\n",
       "      <td>7.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>3.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>3.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>every</td>\n",
       "      <td>7.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>power</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>than</td>\n",
       "      <td>7.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>3.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>continue</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>3.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>discussed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>longer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>26.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>company</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>unsubscribe</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>year</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>california</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>didnt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>22.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>provided</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>even</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>june</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>friends</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>doesnt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>less</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>trading</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>win</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>fixed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>field</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>ever</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>air</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>idea</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>software</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>control</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>draft</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>employees</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>proposed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>general</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>video</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>inbox</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>whats</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>included</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>members</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>currently</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>asked</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>unfortunately</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>gift</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646</th>\n",
       "      <td>million</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>addresses</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>648</th>\n",
       "      <td>actually</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>costs</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word  yes_intent  no_intent  yes_ratio  no_ratio\n",
       "147        was        24.0       79.0   0.303797  3.291667\n",
       "150        day        23.0       78.0   0.294872  3.391304\n",
       "186    receive        16.0       56.0   0.285714  3.500000\n",
       "223         no        14.0       50.0   0.280000  3.571429\n",
       "224         he        14.0       50.0   0.280000  3.571429\n",
       "..         ...         ...        ...        ...       ...\n",
       "645       gift         1.0        9.0   0.111111  9.000000\n",
       "646    million         1.0        9.0   0.111111  9.000000\n",
       "647  addresses         1.0        9.0   0.111111  9.000000\n",
       "648   actually         1.0        9.0   0.111111  9.000000\n",
       "649      costs         1.0        9.0   0.111111  9.000000\n",
       "\n",
       "[171 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words[common_words['no_ratio']>3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLOTTING THE INSIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Add seaborn plots for the common words\"\"\"\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"browser\"\n",
    "def generate_figure(data): \n",
    "    \n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "\n",
    "    fig.add_trace(go.Bar(x=data['Word'],\n",
    "                     y =data['yes_intent'],  name = 'Intent'),\n",
    "                     secondary_y = False)\n",
    "    \n",
    "    fig.add_trace(go.Bar(x=data['Word'],\n",
    "                     y =data['no_intent'],  name = 'No Intent'),\n",
    "                     secondary_y = False)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=data['Word'],\n",
    "                     y =data['yes_ratio'],  name = 'Intent Ratio'),\n",
    "                     secondary_y = True)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=data['Word'],\n",
    "                     y =data['no_ratio'],  name = 'No Intent Ratio'),\n",
    "                     secondary_y = True)\n",
    "    \n",
    "    \n",
    "    fig.update_yaxes(title_text=\"<b>WORD COUNT</b>\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"<b>WORD RELATIVE RATIO</b>\", secondary_y=True)\n",
    "\n",
    "    return fig\n",
    "\n",
    "generate_figure(common_words).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VECTORIZING THE DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the insights we have gathered so far reflect a trend in the words used in emails having \"intent\", we must formalize the process of identify these emails. To do so, we will use statistical models to see if they are effective in identifying the sentiment we are looking for, and in case we are disappointed by the results, we shall seek the even more mathetically complex neural networks to detect trends for us. \n",
    "\n",
    "The first step in this process would be to vectorize the dataset, as words are meaningless to these statistical models and must be converted to numbers. We take the complete corpus and vectorize it using both the count and the tfidf methods, because an initial examination of our data has revealed that some words are more common in the \"yes\" set, while a few others are more common in the \"no\" set. The nature of the tfidf method will automatically give more weight to these important variables that occur less frequently, as opposed to words like \"the\" and \"you\" which are scattered everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"We shall attempt to implement a unigram model, and then a dtidf model to see what trends they\n",
    "reveal about the data\"\"\"\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3), stop_words = ['to', 'you', 'and', 'the', 'a'])\n",
    "tfidf_vectorizer = TfidfVectorizer(norm = 'l2', ngram_range=(3,3), stop_words = ['to', 'you', 'and', 'the', 'a'])\n",
    "\n",
    "xtrain_count = vectorizer.fit_transform(x_train)\n",
    "xtrain_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "xtest_count = vectorizer.transform(x_test)\n",
    "xtest_tfidf = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3657x37939 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 41883 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING THE MODEL(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Naive Bayes model is being trained.\n",
      "Best parameters for Naive Bayes on GridSearch in train data:  {}\n",
      "Train Accuracy for Naive Bayes is: 0.99808586273 for count and 0.99835931091 for tfidf\n",
      "Test Accuracy for Naive Bayes is: 0.74193548387 for count and 0.74697580645 for tfidf\n",
      "The SVC model is being trained.\n",
      "Best parameters for SVC on GridSearch in train data:  {'SVC__loss': 'hinge', 'SVC__tol': 0.0001}\n",
      "Train Accuracy for SVC is: 0.99863275909 for count and 0.99863275909 for tfidf\n",
      "Test Accuracy for SVC is: 0.76612903226 for count and 0.75302419355 for tfidf\n",
      "The Random Forest model is being trained.\n",
      "Best parameters for Random Forest on GridSearch in train data:  {'RF__max_depth': 1000, 'RF__n_estimators': 500}\n",
      "Train Accuracy for Random Forest is: 0.99671862182 for count and 0.77932731747 for tfidf\n",
      "Test Accuracy for Random Forest is: 0.74193548387 for count and 0.74495967742 for tfidf\n",
      "Train Accuracy {('Naive Bayes', 'count'): 0.9980858627290129, ('Naive Bayes', 'tfidf'): 0.9983593109105825, ('SVC', 'count'): 0.998632759092152, ('SVC', 'tfidf'): 0.998632759092152, ('Random Forest', 'count'): 0.9967186218211649, ('Random Forest', 'tfidf'): 0.7793273174733388}\n",
      "Test Accuracy {('Naive Bayes', 'count'): 0.7419354838709677, ('Naive Bayes', 'tfidf'): 0.7469758064516129, ('SVC', 'count'): 0.7661290322580645, ('SVC', 'tfidf'): 0.7530241935483871, ('Random Forest', 'count'): 0.7419354838709677, ('Random Forest', 'tfidf'): 0.7449596774193549}\n"
     ]
    }
   ],
   "source": [
    "models = {\"Naive Bayes\": [(\"naive_bayes\", MultinomialNB())], \"SVC\": [(\"SVC\", LinearSVC())], \n",
    "         \"Random Forest\": [(\"RF\",RandomForestClassifier(n_estimators=1000, max_depth = 500))]}\n",
    "predicted = dict()\n",
    "accuracy = dict(dict())\n",
    "test_predicted = dict()\n",
    "test_accuracy = dict()\n",
    "\n",
    "parameters = {'Naive Bayes': {}, \n",
    "              'SVC': {\"SVC__tol\": [1e-04, 1e-05], \"SVC__loss\": ['hinge', 'squared_hinge']}, \n",
    "              'Random Forest': {\"RF__n_estimators\": [200,500], \"RF__max_depth\": [500, 1000]} }\n",
    "\n",
    "for model in models: \n",
    "    \n",
    "    print(\"The %s model is being trained.\"%model)\n",
    "    \n",
    "    pipe = Pipeline(models[model])\n",
    "    params = parameters[model]\n",
    "    clf = GridSearchCV(pipe, param_grid = params, n_jobs = -1, cv = 5).fit(xtrain_count, y_train)\n",
    "    clf_tfidf = GridSearchCV(pipe, param_grid = params, cv = 5).fit(xtrain_tfidf, y_train)\n",
    "    print(\"Best parameters for %s on GridSearch in train data: \"%(model), clf.best_params_)\n",
    "    \n",
    "    #train predictions, will make loop later\n",
    "    predicted[model, 'count'] = clf.predict(xtrain_count)\n",
    "    predicted[model, 'tfidf'] = clf_tfidf.predict(xtrain_tfidf)\n",
    "    accuracy[model, 'count'] = accuracy_score(y_train, predicted[model, 'count'])\n",
    "    accuracy[model, 'tfidf'] = accuracy_score(y_train, predicted[model, 'tfidf'])\n",
    "    \n",
    "    #test predictions, will make loop later\n",
    "    test_predicted[model, 'count'] = clf.predict(xtest_count)\n",
    "    test_predicted[model, 'tfidf'] = clf_tfidf.predict(xtest_tfidf)\n",
    "    test_accuracy[model, 'count'] = accuracy_score(y_test, test_predicted[model, 'count'])\n",
    "    test_accuracy[model, 'tfidf'] = accuracy_score(y_test, test_predicted[model, 'tfidf'])\n",
    "    print(\"Train Accuracy for %s is: %1.11f for count and %1.11f for tfidf\"%(model, \n",
    "                        accuracy[model, 'count'], accuracy[model, 'tfidf']))\n",
    "    print(\"Test Accuracy for %s is: %1.11f for count and %1.11f for tfidf\"%(model, \n",
    "                        test_accuracy[model, 'count'], test_accuracy[model, 'tfidf']))\n",
    "    \n",
    "    \n",
    "print(\"Train Accuracy\", accuracy) \n",
    "print(\"Test Accuracy\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLING THE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when combining count results: 0.7399193548387096\n"
     ]
    }
   ],
   "source": [
    "\"\"\"We will now take the results obtained from each of the models and merge them to see if the accuracy is higher. \n",
    "We have three methods that we have used currently, and our final result will take the opinion of two or more.\"\"\"\n",
    "\n",
    "# Results for the Count Vector\n",
    "\n",
    "merged_results = list(zip(test_predicted['Naive Bayes', 'count'], \n",
    "                          test_predicted['Random Forest', 'count'], test_predicted['SVC', 'count']))\n",
    "added_results = [sum(x) for x in merged_results]\n",
    "final_results = [1 if x > 2 else 0 for x in added_results]\n",
    "print(\"Accuracy when combining count results:\",accuracy_score(final_results, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when combining the tfidf results: 0.7429435483870968\n"
     ]
    }
   ],
   "source": [
    "# Results for the tfidf Vector\n",
    "\n",
    "merged_results = list(zip(test_predicted['Naive Bayes', 'tfidf'], \n",
    "                          test_predicted['Random Forest', 'tfidf'], test_predicted['SVC', 'tfidf']))\n",
    "added_results = [sum(x) for x in merged_results]\n",
    "final_results = [1 if x > 2 else 0 for x in added_results]\n",
    "print(\"Accuracy when combining the tfidf results:\",accuracy_score(final_results, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy when combining the count and tfidf results: 0.7389112903225806\n"
     ]
    }
   ],
   "source": [
    "# Results for the count and tfidf vectors combined\n",
    "\n",
    "merged_results = list(zip(test_predicted['Naive Bayes', 'count'], test_predicted['Random Forest', 'count'], \n",
    "                          test_predicted['SVC', 'count'], test_predicted['Naive Bayes', 'tfidf'], \n",
    "                          test_predicted['Random Forest', 'tfidf'], test_predicted['SVC', 'tfidf']))\n",
    "added_results = [sum(x) for x in merged_results]\n",
    "final_results = [1 if x > 5 else 0 for x in added_results]\n",
    "print(\"Accuracy when combining the count and tfidf results:\",accuracy_score(final_results, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIAL INFERENCES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after preprocessing, examining, and running the data against some benchmark classification models, we notice that while accuracy in training data is nearly 100 percent, the same cannot be said about test data, which is close to the 75-80 percent range. Some enhancements in the results come after parameter tuning using grid search in sklearn, cleaning data for punctuation and stop words, using bi-grams and tri-grams while vectorizing and ensembling the results at the end. However, the model is still overfit, and we need to explore other methods that might increase the efficiency of identifying intent. One such solution is using a neural network, that might be able to identify some patterns in the largely sparse matrix that other machine learning models have been unable to so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGS TO EXPLORE\n",
    "\n",
    "1. Data Resampling\n",
    "2. Error Analysis (seeing where the mistakes are and if accuracy can be improved)\n",
    "3. Other methods for encoding\n",
    "4. Hyperparameter tuning using gridsearch\n",
    "5. Closer analysis of the data to check for normalization\n",
    "6. The random state!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
